{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18903755",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\choij\\anaconda3\\lib\\site-packages\\dask\\dataframe\\utils.py:369: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n",
      "C:\\Users\\choij\\anaconda3\\lib\\site-packages\\dask\\dataframe\\utils.py:369: FutureWarning: pandas.Float64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n",
      "C:\\Users\\choij\\anaconda3\\lib\\site-packages\\dask\\dataframe\\utils.py:369: FutureWarning: pandas.UInt64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n"
     ]
    }
   ],
   "source": [
    "# Data Wrangling\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib import font_manager, rc\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# EDA\n",
    "# import klib\n",
    "\n",
    "# Preprocessing & Feature Engineering\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn import base\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.experimental import enable_iterative_imputer  # still experimental \n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "\n",
    "# Hyperparameter Optimization\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Modeling\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.svm import SVR\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Utility\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import sys, warnings\n",
    "if not sys.warnoptions: warnings.simplefilter(\"ignore\")\n",
    "from IPython.display import Image\n",
    "# import pickle\n",
    "from tqdm import tqdm\n",
    "import platform\n",
    "from itertools import combinations\n",
    "from scipy.stats.mstats import gmean\n",
    "from tensorflow import keras\n",
    "\n",
    "# from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0c99b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22534f69",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a05a072",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e0302b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features_train = pd.read_csv(os.path.abspath(\"../input\")+\"/choi_num_features_train.csv\" , encoding = 'utf-8')\n",
    "num_features_test = pd.read_csv(os.path.abspath(\"../input\")+\"/choi_num_features_test.csv\" , encoding = 'utf-8')\n",
    "onehot_features_train = pd.read_csv(os.path.abspath(\"../input\")+'/choi_onehot_features_train.csv' , encoding = 'utf-8')\n",
    "onehot_features_test = pd.read_csv(os.path.abspath(\"../input\")+'/choi_onehot_features_test.csv' , encoding = 'utf-8')\n",
    "w2v_features_train = pd.read_csv(os.path.abspath(\"../input\")+'/choi_w2v_features_train.csv' , encoding = 'utf-8')\n",
    "w2v_features_test = pd.read_csv(os.path.abspath(\"../input\")+'/choi_w2v_features_test.csv' , encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "332ed0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_id = num_features_train.custid.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991ba8b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa22f371",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat([num_features_train,onehot_features_train], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41372454",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.concat([num_features_test,onehot_features_test], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b05dde",
   "metadata": {},
   "source": [
    "# kmean 클러스터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32edc4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "features__ = X_train.drop(columns='custid',axis=1).fillna(0)\n",
    "features_t__= X_test.drop(columns='custid',axis=1).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85063b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "class KMeansFeaturizer:\n",
    "    \"\"\" 숫자 데이터를 k-평균 클러스터 멤버십으로 변환.\n",
    "\n",
    "    이 변환기는 입력 데이터에 k-평균을 수행해 각 데이터 포인트를 가장 가까운 클러스터의 id로 변환한다.\n",
    "    만약 목표 변수가 주어지면 유사한 데이터 포인트와 함께 grouping되고,\n",
    "    분류 경계에 따르는 클러스터를 생성하기 위해 스케일링되고, k-평균 입력에 포함된다.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, k = 100, target_scale = 5.0, random_state = None):\n",
    "        self.k = k\n",
    "        self.target_scale = target_scale\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y = None):\n",
    "        \"\"\" 입력 데이터에 k-평균을 수행하고 중심점을 찾는다.\n",
    "        \"\"\"\n",
    "        if y is None: # 목표 변수가 없으면 단순한 k-평균 수행\n",
    "            km_model = KMeans(n_clusters = self.k, n_init = 20, random_state = self.random_state)\n",
    "            km_model.fit(X)\n",
    "            # n_inie -> runtime\n",
    "            self.inertia_ = km_model.inertia_ # 가장 가까운 center 와의 거리\n",
    "            self.km_model = km_model\n",
    "            self.cluster_centers_ = km_model.cluster_centers_\n",
    "            return self\n",
    "\n",
    "        # 목표 변수가 있으면, 적절한 스케일링을 적용하고, 이를 k-평균에 대한 입력 데이터에 포함시킨다.\n",
    "        data_with_target = np.hstack((X, y[:, np.newaxis] * self.target_scale))\n",
    "        # 데이터와 타겟에 대해 사전 학습할 k-평균 모델 구축\n",
    "        km_model_pretrain = KMeans(n_clusters = self.k, n_init = 20, random_state = self.random_state)\n",
    "        km_model_pretrain.fit(data_with_target)\n",
    "\n",
    "        # k평균을 두번째로 실행해 목표 변수 없이 원시 공간에서 클러스터를 얻는다. 사전 학습을 통해 얻은 중심점을 활용해 초기화한다.\n",
    "        # 반복을 통해 클러스터 할당과 중심점 계산을 다시 수행한다.\n",
    "\n",
    "        km_model = KMeans(n_clusters = self.k, init = km_model_pretrain.cluster_centers_[:,:data_with_target.shape[1]-1], n_init = 1, max_iter = 1)\n",
    "\n",
    "        km_model.fit(X)\n",
    "        \n",
    "        self.inertia_ = km_model.inertia_\n",
    "        self.km_model = km_model\n",
    "        self.cluster_centers_ = km_model.cluster_centers_\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y = None):\n",
    "        \"\"\" 각 입력 데이터 포인트에 대해 가장 가까운 클러스터 ID 산출\n",
    "        \"\"\"\n",
    "        clusters = self.km_model.predict(X)\n",
    "        return clusters[:, np.newaxis]\n",
    "\n",
    "    def fit_transform(self, X, y = None):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2cd8598",
   "metadata": {},
   "outputs": [],
   "source": [
    "kme = KMeansFeaturizer(random_state = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c074b55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features_k = kme.fit_transform(features__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffca6981",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_k_t = kme.transform(features_t__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fea6bfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_kmean = pd.concat([pd.DataFrame(features_k),pd.DataFrame(features_k_t)],axis = 0).reset_index().drop(\"index\",axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb16f067",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_kmean = features_kmean.astype(\"str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf4b26af",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_k_dum = pd.get_dummies(features_kmean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ecdaabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_col = features_k_dum.shape[1]\n",
    "pca = PCA(n_components=max_col, random_state=0).fit(features_k_dum)\n",
    "\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "num_col = np.argmax(cumsum >= 0.99) + 1\n",
    "\n",
    "\n",
    "if num_col == 1:\n",
    "    num_col = max_col\n",
    "\n",
    "pca = PCA(n_components = num_col, random_state=0).fit_transform(features_k_dum)\n",
    "features_k_dum = pd.DataFrame(pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d4cbd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_k_train = features_k_dum[:len(features__)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f06f7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_k_test = features_k_dum[len(features__):].reset_index().drop(\"index\",axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef5acf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_k_train.to_csv('choi_features_k_train.csv', index=False)\n",
    "features_k_test.to_csv('choi_features_k_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5924a9",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81ff056",
   "metadata": {},
   "source": [
    "# kmean 클러스터링(only Numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59850b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "features__ = num_features_train.drop(columns='custid',axis=1).fillna(0)\n",
    "features_t__= num_features_test.drop(columns='custid',axis=1).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe39ba6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "class KMeansFeaturizer:\n",
    "    \"\"\" 숫자 데이터를 k-평균 클러스터 멤버십으로 변환.\n",
    "\n",
    "    이 변환기는 입력 데이터에 k-평균을 수행해 각 데이터 포인트를 가장 가까운 클러스터의 id로 변환한다.\n",
    "    만약 목표 변수가 주어지면 유사한 데이터 포인트와 함께 grouping되고,\n",
    "    분류 경계에 따르는 클러스터를 생성하기 위해 스케일링되고, k-평균 입력에 포함된다.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, k = 100, target_scale = 5.0, random_state = None):\n",
    "        self.k = k\n",
    "        self.target_scale = target_scale\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y = None):\n",
    "        \"\"\" 입력 데이터에 k-평균을 수행하고 중심점을 찾는다.\n",
    "        \"\"\"\n",
    "        if y is None: # 목표 변수가 없으면 단순한 k-평균 수행\n",
    "            km_model = KMeans(n_clusters = self.k, n_init = 20, random_state = self.random_state)\n",
    "            km_model.fit(X)\n",
    "            \n",
    "            self.inertia_ = km_model.inertia_\n",
    "            self.km_model = km_model\n",
    "            self.cluster_centers_ = km_model.cluster_centers_\n",
    "            return self\n",
    "\n",
    "        # 목표 변수가 있으면, 적절한 스케일링을 적용하고, 이를 k-평균에 대한 입력 데이터에 포함시킨다.\n",
    "        data_with_target = np.hstack((X, y[:, np.newaxis] * self.target_scale))\n",
    "        # 데이터와 타겟에 대해 사전 학습할 k-평균 모델 구축\n",
    "        km_model_pretrain = KMeans(n_clusters = self.k, n_init = 20, random_state = self.random_state)\n",
    "        km_model_pretrain.fit(data_with_target)\n",
    "\n",
    "        # k평균을 두번째로 실행해 목표 변수 없이 원시 공간에서 클러스터를 얻는다. 사전 학습을 통해 얻은 중심점을 활용해 초기화한다.\n",
    "        # 반복을 통해 클러스터 할당과 중심점 계산을 다시 수행한다.\n",
    "\n",
    "        km_model = KMeans(n_clusters = self.k, init = km_model_pretrain.cluster_centers_[:,:data_with_target.shape[1]-1], n_init = 1, max_iter = 1)\n",
    "\n",
    "        km_model.fit(X)\n",
    "        \n",
    "        self.inertia_ = km_model.inertia_\n",
    "        self.km_model = km_model\n",
    "        self.cluster_centers_ = km_model.cluster_centers_\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y = None):\n",
    "        \"\"\" 각 입력 데이터 포인트에 대해 가장 가까운 클러스터 ID 산출\n",
    "        \"\"\"\n",
    "        clusters = self.km_model.predict(X)\n",
    "        return clusters[:, np.newaxis]\n",
    "\n",
    "    def fit_transform(self, X, y = None):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c174486",
   "metadata": {},
   "outputs": [],
   "source": [
    "kme = KMeansFeaturizer(random_state = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e6d3135",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features_k = kme.fit_transform(features__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "65090a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_k_t = kme.transform(features_t__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0f59c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_kmean = pd.concat([pd.DataFrame(features_k),pd.DataFrame(features_k_t)],axis = 0).reset_index().drop(\"index\",axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "34f0e27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_kmean = features_kmean.astype(\"str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "109179d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_k_dum = pd.get_dummies(features_kmean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d4b68a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_col = features_k_dum.shape[1]\n",
    "pca = PCA(n_components=max_col, random_state=0).fit(features_k_dum)\n",
    "\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "num_col = np.argmax(cumsum >= 0.99) + 1\n",
    "\n",
    "\n",
    "if num_col == 1:\n",
    "    num_col = max_col\n",
    "\n",
    "pca = PCA(n_components = num_col, random_state=0).fit_transform(features_k_dum)\n",
    "features_k_dum = pd.DataFrame(pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "449d15cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_k_train_num = features_k_dum[:len(features__)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5ea366f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_k_test_num = features_k_dum[len(features__):].reset_index().drop(\"index\",axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d270ff76",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_k_train_num.to_csv('choi_features_k_train_num.csv', index=False)\n",
    "features_k_test_num.to_csv('choi_features_k_test_num.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0714742e",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
