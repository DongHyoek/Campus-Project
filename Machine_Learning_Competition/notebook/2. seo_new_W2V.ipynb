{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "022a361e",
   "metadata": {},
   "source": [
    "# 미리 만들어놨던 W2V 파일 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1146b19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\inuya\\anaconda3\\lib\\site-packages\\xgboost\\compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "# !pip install catboost\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# !pip install sweetviz\n",
    "import sweetviz as sv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import Series, DataFrame\n",
    "from matplotlib import font_manager, rc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "from matplotlib import cm\n",
    "import matplotlib.font_manager as fm\n",
    "rc('font',family= 'malgun gothic')\n",
    "rc('axes', unicode_minus = False)\n",
    "\n",
    "import math\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "# !pip install -U klib\n",
    "import klib\n",
    "import time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import sys, warnings\n",
    "if not sys.warnoptions: warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "from sklearn.impute import SimpleImputer \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.base import ClassifierMixin\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "from IPython.display import Image\n",
    "import pickle\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import platform\n",
    "from itertools import combinations\n",
    "from scipy.stats.mstats import gmean\n",
    "\n",
    "\n",
    "import datetime\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ae01fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8775ca5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(os.path.abspath(\"../input\")+'/X_train.csv', encoding = 'cp949')\n",
    "test = pd.read_csv(os.path.abspath(\"../input\")+'/X_test.csv', encoding = 'cp949')\n",
    "y_train = pd.read_csv(os.path.abspath(\"../input\")+'/y_train.csv', encoding = 'cp949')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ff6f3efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([train, test]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "28573ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting word2vec_list.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile word2vec_list.py\n",
    "\n",
    "### Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "### Read data\n",
    "train = pd.read_csv(os.path.abspath(\"../input\")+'/X_train.csv', encoding='cp949')\n",
    "test = pd.read_csv(os.path.abspath(\"../input\")+'/X_test.csv', encoding='cp949')\n",
    "\n",
    "# str_nm ~ buyer_nm까지 하나의 문장으로 만들어보기.\n",
    "cat_list = ['str_nm','goodcd','brd_nm','corner_nm','pc_nm','part_nm','team_nm','buyer_nm']\n",
    "# train data\n",
    "w2v_train = train.loc[:,cat_list]\n",
    "w2v_train.goodcd = w2v_train.goodcd.astype(str)\n",
    "w2v_train['list'] = w2v_train.apply(lambda x : list(x),axis = 1)\n",
    "w2v_train['list'] = w2v_train.list.apply(lambda x: ''.join(map(str,x)))\n",
    "\n",
    "# test data\n",
    "w2v_test = test.loc[:,cat_list]\n",
    "w2v_test.goodcd = w2v_test.goodcd.astype(str)\n",
    "w2v_test['list'] = w2v_test.apply(lambda x: list(x),axis = 1)\n",
    "w2v_test['list'] = w2v_test.list.apply(lambda x: ''.join(map(str,x)))\n",
    "\n",
    "# train/test data에 추가\n",
    "train['list'] = w2v_train.list\n",
    "test['list'] = w2v_test.list\n",
    "\n",
    "### Make corpus\n",
    "p_level = 'list'  # 상품 분류 수준\n",
    "\n",
    "# W2V 학습데이터가 부족하여 구매한 상품 목록으로부터 n배 oversampling을 수행\n",
    "def oversample(x, n, seed=0):\n",
    "    if n == 0:\n",
    "        return list(x)\n",
    "    uw = np.unique(x) # unique 값들 array형태\n",
    "    bs = np.array([]) # 빈 array 생성\n",
    "    np.random.seed(seed) # seed 지정\n",
    "    for j in range(n):\n",
    "        bs = np.append(bs, np.random.choice(uw, len(uw), replace=True))  # 복원추출\n",
    "    return list(bs)\n",
    "\n",
    "train_corpus = list(train.groupby('custid')[p_level].agg(oversample, 20))\n",
    "test_corpus = list(test.groupby('custid')[p_level].agg(oversample, 20))\n",
    "\n",
    "\n",
    "### Training the Word2Vec model\n",
    "num_features = 100 # 단어 벡터 차원 수\n",
    "min_word_count = 1 # 최소 단어 수\n",
    "context = 5 # 학습 윈도우(인접한 단어 리스트) 크기\n",
    "\n",
    "# 초기화 및 모델 학습\n",
    "from gensim.models import word2vec\n",
    "\n",
    "# 모델 학습\n",
    "w2v = word2vec.Word2Vec(train_corpus, \n",
    "                        vector_size=num_features, \n",
    "                        min_count=min_word_count,\n",
    "                        window=context,\n",
    "                        seed=0, workers=1)\n",
    "# 필요없는 메모리 unload\n",
    "w2v.init_sims(replace=True)\n",
    "\n",
    "\n",
    "### Make features\n",
    "# 구매상품에 해당하는 벡터의 평균/최소/최대 벡터를 feature로 만드는 전처리기\n",
    "class EmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.dim = num_features\n",
    "    def fit(self, X):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.hstack([\n",
    "                np.max([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0),\n",
    "                np.min([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0),\n",
    "                np.mean([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0),                \n",
    "                #np.std([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0)                \n",
    "            ]) \n",
    "            for words in X\n",
    "        ]) \n",
    "\n",
    "# W2V 기반 feature 생성\n",
    "train_features = pd.DataFrame(EmbeddingVectorizer(w2v.wv).fit(train_corpus).transform(train_corpus))\n",
    "test_features = pd.DataFrame(EmbeddingVectorizer(w2v.wv).transform(test_corpus))\n",
    "\n",
    "train_features.columns = ['v'+f'{c+1:03d}' for c in train_features.columns]\n",
    "test_features.columns = ['v'+f'{c+1:03d}' for c in test_features.columns]\n",
    "\n",
    "# 학습용과 제출용 데이터로 분리\n",
    "X_train_list = pd.concat([pd.DataFrame({'custid': np.sort(train['custid'].unique())}), train_features], axis=1)#.to_csv('X_train_buyer.csv', index=False)\n",
    "X_test_list = pd.concat([pd.DataFrame({'custid': np.sort(test['custid'].unique())}), test_features], axis=1)#.to_csv('X_test_buyer.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "3167dcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run word2vec_list.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c60afc0",
   "metadata": {},
   "source": [
    "### Feature Merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275bfcd8",
   "metadata": {},
   "source": [
    "#### 하나의 문장 list로 만든 word2vec 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "fd3aaed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_features_train = pd.read_csv(os.path.abspath('../input') + '/seo_w2v_features_train.csv',encoding = 'utf-8')\n",
    "w2v_features_test = pd.read_csv(os.path.abspath('../input') + '/seo_w2v_features_test.csv',encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "9cb18b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train_list['custid']\n",
    "del X_test_list['custid']\n",
    "\n",
    "X_train_list.columns = X_train_list.columns.map(lambda x : \"list_\" + str(x))\n",
    "X_test_list.columns = X_test_list.columns.map(lambda x : \"list_\" + str(x))\n",
    "\n",
    "w2v_features_train_new = pd.concat([w2v_features_train,X_train_list],axis = 1)\n",
    "w2v_features_test_new = pd.concat([w2v_features_test,X_test_list],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "88646fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_features_train_new.to_csv('seo_w2v_features_train_new.csv', index=False,encoding = 'utf-8')\n",
    "w2v_features_test_new.to_csv('seo_w2v_features_test_new.csv', index=False,encoding = 'utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
