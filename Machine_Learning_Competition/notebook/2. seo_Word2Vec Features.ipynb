{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1146b19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\inuya\\anaconda3\\lib\\site-packages\\xgboost\\compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "# !pip install catboost\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# !pip install sweetviz\n",
    "import sweetviz as sv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import Series, DataFrame\n",
    "from matplotlib import font_manager, rc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "from matplotlib import cm\n",
    "import matplotlib.font_manager as fm\n",
    "rc('font',family= 'malgun gothic')\n",
    "rc('axes', unicode_minus = False)\n",
    "\n",
    "import math\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "# !pip install -U klib\n",
    "import klib\n",
    "import time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import sys, warnings\n",
    "if not sys.warnoptions: warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "from sklearn.impute import SimpleImputer \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.base import ClassifierMixin\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "from IPython.display import Image\n",
    "import pickle\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import platform\n",
    "from itertools import combinations\n",
    "from scipy.stats.mstats import gmean\n",
    "\n",
    "\n",
    "import datetime\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ae01fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8775ca5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(os.path.abspath(\"../input\")+'/X_train.csv', encoding = 'cp949')\n",
    "test = pd.read_csv(os.path.abspath(\"../input\")+'/X_test.csv', encoding = 'cp949')\n",
    "y_train = pd.read_csv(os.path.abspath(\"../input\")+'/y_train.csv', encoding = 'cp949')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff6f3efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([train, test]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92c79359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing word2vec_corner.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%writefile word2vec_corner.py\n",
    "\n",
    "### Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "### Read data\n",
    "train = pd.read_csv(os.path.abspath(\"../input\")+'/X_train.csv', encoding='cp949')\n",
    "test = pd.read_csv(os.path.abspath(\"../input\")+'/X_test.csv', encoding='cp949')\n",
    "\n",
    "\n",
    "### Make corpus\n",
    "p_level = 'corner_nm'  # 상품 분류 수준\n",
    "\n",
    "# W2V 학습데이터가 부족하여 구매한 상품 목록으로부터 n배 oversampling을 수행\n",
    "def oversample(x, n, seed=0):\n",
    "    if n == 0:\n",
    "        return list(x)\n",
    "    uw = np.unique(x)\n",
    "    bs = np.array([])\n",
    "    np.random.seed(seed)\n",
    "    for j in range(n):\n",
    "        bs = np.append(bs, np.random.choice(uw, len(uw), replace=True))  # 복원추출\n",
    "    return list(bs)\n",
    "\n",
    "train_corpus = list(train.groupby('custid')[p_level].agg(oversample, 20))\n",
    "test_corpus = list(test.groupby('custid')[p_level].agg(oversample, 20))\n",
    "\n",
    "\n",
    "### Training the Word2Vec model\n",
    "num_features = 100 # 단어 벡터 차원 수\n",
    "min_word_count = 1 # 최소 단어 수\n",
    "context = 5 # 학습 윈도우(인접한 단어 리스트) 크기\n",
    "\n",
    "# 초기화 및 모델 학습\n",
    "from gensim.models import word2vec\n",
    "\n",
    "# 모델 학습\n",
    "w2v = word2vec.Word2Vec(train_corpus, \n",
    "                        vector_size=num_features, \n",
    "                        min_count=min_word_count,\n",
    "                        window=context,\n",
    "                        seed=0, workers=1)\n",
    "# 필요없는 메모리 unload\n",
    "w2v.init_sims(replace=True)\n",
    "\n",
    "\n",
    "### Make features\n",
    "# 구매상품에 해당하는 벡터의 평균/최소/최대 벡터를 feature로 만드는 전처리기\n",
    "class EmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.dim = num_features\n",
    "    def fit(self, X):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.hstack([\n",
    "                np.max([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0),\n",
    "                np.min([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0),\n",
    "                np.mean([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0),                \n",
    "                #np.std([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0)                \n",
    "            ]) \n",
    "            for words in X\n",
    "        ]) \n",
    "\n",
    "# W2V 기반 feature 생성\n",
    "train_features = pd.DataFrame(EmbeddingVectorizer(w2v.wv).fit(train_corpus).transform(train_corpus))\n",
    "test_features = pd.DataFrame(EmbeddingVectorizer(w2v.wv).transform(test_corpus))\n",
    "\n",
    "train_features.columns = ['v'+f'{c+1:03d}' for c in train_features.columns]\n",
    "test_features.columns = ['v'+f'{c+1:03d}' for c in test_features.columns]\n",
    "\n",
    "# 학습용과 제출용 데이터로 분리\n",
    "X_train_corner = pd.concat([pd.DataFrame({'custid': np.sort(train['custid'].unique())}), train_features], axis=1)#.to_csv('X_train_buyer.csv', index=False)\n",
    "X_test_corner = pd.concat([pd.DataFrame({'custid': np.sort(test['custid'].unique())}), test_features], axis=1)#.to_csv('X_test_buyer.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfe07dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run word2vec_corner.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9ab33fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing word2vec_brd.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%writefile word2vec_brd.py\n",
    "\n",
    "### Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "### Read data\n",
    "train = pd.read_csv(os.path.abspath(\"../input\")+'/X_train.csv', encoding='cp949')\n",
    "test = pd.read_csv(os.path.abspath(\"../input\")+'/X_test.csv', encoding='cp949')\n",
    "\n",
    "\n",
    "### Make corpus\n",
    "p_level = 'brd_nm'  # 상품 분류 수준\n",
    "\n",
    "# W2V 학습데이터가 부족하여 구매한 상품 목록으로부터 n배 oversampling을 수행\n",
    "def oversample(x, n, seed=0):\n",
    "    if n == 0:\n",
    "        return list(x)\n",
    "    uw = np.unique(x)\n",
    "    bs = np.array([])\n",
    "    np.random.seed(seed)\n",
    "    for j in range(n):\n",
    "        bs = np.append(bs, np.random.choice(uw, len(uw), replace=True))\n",
    "    return list(bs)\n",
    "\n",
    "train_corpus = list(train.groupby('custid')[p_level].agg(oversample, 20))\n",
    "test_corpus = list(test.groupby('custid')[p_level].agg(oversample, 20))\n",
    "\n",
    "\n",
    "### Training the Word2Vec model\n",
    "num_features = 300 # 단어 벡터 차원 수\n",
    "min_word_count = 1 # 최소 단어 수\n",
    "context = 5 # 학습 윈도우(인접한 단어 리스트) 크기\n",
    "\n",
    "# 초기화 및 모델 학습\n",
    "from gensim.models import word2vec\n",
    "\n",
    "# 모델 학습\n",
    "w2v = word2vec.Word2Vec(train_corpus, \n",
    "                        vector_size=num_features, \n",
    "                        min_count=min_word_count,\n",
    "                        window=context,\n",
    "                        seed=0, workers=1)\n",
    "# 필요없는 메모리 unload\n",
    "w2v.init_sims(replace=True)\n",
    "\n",
    "\n",
    "### Make features\n",
    "# 구매상품에 해당하는 벡터의 평균/최소/최대 벡터를 feature로 만드는 전처리기\n",
    "class EmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.dim = num_features\n",
    "    def fit(self, X):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.hstack([\n",
    "                np.max([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0),\n",
    "                np.min([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0),\n",
    "                np.mean([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0),                \n",
    "                #np.std([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0)                \n",
    "            ]) \n",
    "            for words in X\n",
    "        ]) \n",
    "\n",
    "# W2V 기반 feature 생성\n",
    "train_features = pd.DataFrame(EmbeddingVectorizer(w2v.wv).fit(train_corpus).transform(train_corpus))\n",
    "test_features = pd.DataFrame(EmbeddingVectorizer(w2v.wv).transform(test_corpus))\n",
    "\n",
    "train_features.columns = ['v'+f'{c+1:03d}' for c in train_features.columns]\n",
    "test_features.columns = ['v'+f'{c+1:03d}' for c in test_features.columns]\n",
    "\n",
    "# 학습용과 제출용 데이터로 분리\n",
    "X_train_brd = pd.concat([pd.DataFrame({'custid': np.sort(train['custid'].unique())}), train_features], axis=1)#.to_csv('X_train_buyer.csv', index=False)\n",
    "X_test_brd = pd.concat([pd.DataFrame({'custid': np.sort(test['custid'].unique())}), test_features], axis=1)#.to_csv('X_test_buyer.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "728039bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run word2vec_brd.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30702c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing word2vec_pc.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%writefile word2vec_pc.py\n",
    "\n",
    "### Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "### Read data\n",
    "train = pd.read_csv(os.path.abspath(\"../input\")+'/X_train.csv', encoding='cp949')\n",
    "test = pd.read_csv(os.path.abspath(\"../input\")+'/X_test.csv', encoding='cp949')\n",
    "\n",
    "\n",
    "### Make corpus\n",
    "p_level = 'pc_nm'  # 상품 분류 수준\n",
    "\n",
    "# W2V 학습데이터가 부족하여 구매한 상품 목록으로부터 n배 oversampling을 수행\n",
    "def oversample(x, n, seed=0):\n",
    "    if n == 0:\n",
    "        return list(x)\n",
    "    uw = np.unique(x)\n",
    "    bs = np.array([])\n",
    "    np.random.seed(seed)\n",
    "    for j in range(n):\n",
    "        bs = np.append(bs, np.random.choice(uw, len(uw), replace=True))\n",
    "    return list(bs)\n",
    "\n",
    "train_corpus = list(train.groupby('custid')[p_level].agg(oversample, 20))\n",
    "test_corpus = list(test.groupby('custid')[p_level].agg(oversample, 20))\n",
    "\n",
    "\n",
    "### Training the Word2Vec model\n",
    "num_features = 50 # 단어 벡터 차원 수\n",
    "min_word_count = 1 # 최소 단어 수\n",
    "context = 5 # 학습 윈도우(인접한 단어 리스트) 크기\n",
    "\n",
    "# 초기화 및 모델 학습\n",
    "from gensim.models import word2vec\n",
    "\n",
    "# 모델 학습\n",
    "w2v = word2vec.Word2Vec(train_corpus, \n",
    "                        vector_size=num_features, \n",
    "                        min_count=min_word_count,\n",
    "                        window=context,\n",
    "                        seed=0, workers=1)\n",
    "# 필요없는 메모리 unload\n",
    "w2v.init_sims(replace=True)\n",
    "\n",
    "\n",
    "### Make features\n",
    "# 구매상품에 해당하는 벡터의 평균/최소/최대 벡터를 feature로 만드는 전처리기\n",
    "class EmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.dim = num_features\n",
    "    def fit(self, X):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.hstack([\n",
    "                np.max([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0),\n",
    "                np.min([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0),\n",
    "                np.mean([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0),                \n",
    "                #np.std([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0)                \n",
    "            ]) \n",
    "            for words in X\n",
    "        ]) \n",
    "\n",
    "# W2V 기반 feature 생성\n",
    "train_features = pd.DataFrame(EmbeddingVectorizer(w2v.wv).fit(train_corpus).transform(train_corpus))\n",
    "test_features = pd.DataFrame(EmbeddingVectorizer(w2v.wv).transform(test_corpus))\n",
    "\n",
    "train_features.columns = ['v'+f'{c+1:03d}' for c in train_features.columns]\n",
    "test_features.columns = ['v'+f'{c+1:03d}' for c in test_features.columns]\n",
    "\n",
    "# 학습용과 제출용 데이터로 분리\n",
    "X_train_pc = pd.concat([pd.DataFrame({'custid': np.sort(train['custid'].unique())}), train_features], axis=1)#.to_csv('X_train_buyer.csv', index=False)\n",
    "X_test_pc = pd.concat([pd.DataFrame({'custid': np.sort(test['custid'].unique())}), test_features], axis=1)#.to_csv('X_test_buyer.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46b2309e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run word2vec_pc.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49b31be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing word2vec_part.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%writefile word2vec_part.py\n",
    "\n",
    "### Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "### Read data\n",
    "train = pd.read_csv(os.path.abspath(\"../input\")+'/X_train.csv', encoding='cp949')\n",
    "test = pd.read_csv(os.path.abspath(\"../input\")+'/X_test.csv', encoding='cp949')\n",
    "\n",
    "\n",
    "### Make corpus\n",
    "p_level = 'part_nm'  # 상품 분류 수준\n",
    "\n",
    "# W2V 학습데이터가 부족하여 구매한 상품 목록으로부터 n배 oversampling을 수행\n",
    "def oversample(x, n, seed=0):\n",
    "    if n == 0:\n",
    "        return list(x)\n",
    "    uw = np.unique(x)\n",
    "    bs = np.array([])\n",
    "    np.random.seed(seed)\n",
    "    for j in range(n):\n",
    "        bs = np.append(bs, np.random.choice(uw, len(uw), replace=True))\n",
    "    return list(bs)\n",
    "\n",
    "train_corpus = list(train.groupby('custid')[p_level].agg(oversample, 20))\n",
    "test_corpus = list(test.groupby('custid')[p_level].agg(oversample, 20))\n",
    "\n",
    "\n",
    "### Training the Word2Vec model\n",
    "num_features = 100 # 단어 벡터 차원 수\n",
    "min_word_count = 1 # 최소 단어 수\n",
    "context = 5 # 학습 윈도우(인접한 단어 리스트) 크기\n",
    "\n",
    "# 초기화 및 모델 학습\n",
    "from gensim.models import word2vec\n",
    "\n",
    "# 모델 학습\n",
    "w2v = word2vec.Word2Vec(train_corpus, \n",
    "                        vector_size=num_features, \n",
    "                        min_count=min_word_count,\n",
    "                        window=context,\n",
    "                        seed=0, workers=1)\n",
    "# 필요없는 메모리 unload\n",
    "w2v.init_sims(replace=True)\n",
    "\n",
    "\n",
    "### Make features\n",
    "# 구매상품에 해당하는 벡터의 평균/최소/최대 벡터를 feature로 만드는 전처리기\n",
    "class EmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.dim = num_features\n",
    "    def fit(self, X):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.hstack([\n",
    "                np.max([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0),\n",
    "                np.min([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0),\n",
    "                np.mean([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0),                \n",
    "                #np.std([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0)                \n",
    "            ]) \n",
    "            for words in X\n",
    "        ]) \n",
    "\n",
    "# W2V 기반 feature 생성\n",
    "train_features = pd.DataFrame(EmbeddingVectorizer(w2v.wv).fit(train_corpus).transform(train_corpus))\n",
    "test_features = pd.DataFrame(EmbeddingVectorizer(w2v.wv).transform(test_corpus))\n",
    "\n",
    "train_features.columns = ['v'+f'{c+1:03d}' for c in train_features.columns]\n",
    "test_features.columns = ['v'+f'{c+1:03d}' for c in test_features.columns]\n",
    "\n",
    "# 학습용과 제출용 데이터로 분리\n",
    "X_train_part = pd.concat([pd.DataFrame({'custid': np.sort(train['custid'].unique())}), train_features], axis=1)#.to_csv('X_train_buyer.csv', index=False)\n",
    "X_test_part = pd.concat([pd.DataFrame({'custid': np.sort(test['custid'].unique())}), test_features], axis=1)#.to_csv('X_test_buyer.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4193ac9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run word2vec_part.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58403865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing word2vec_customer_info.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%writefile word2vec_customer_info.py\n",
    "\n",
    "### Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "### Read data\n",
    "train = pd.read_csv(os.path.abspath(\"../input\")+'/X_train.csv', encoding='cp949')\n",
    "test = pd.read_csv(os.path.abspath(\"../input\")+'/X_test.csv', encoding='cp949')\n",
    "train['customer_info'] = train['brd_nm'].astype(str) + '_' + train['corner_nm'].astype(str) + '_' + train['pc_nm'].astype(str) + '_' + train['part_nm'].astype(str) + '_' + train['str_nm'].astype(str) + '_' + train['team_nm'].astype(str) + '_' + train['buyer_nm'].astype(str)\n",
    "test['customer_info'] = test['brd_nm'].astype(str) + '_' + test['corner_nm'].astype(str) + '_' + test['pc_nm'].astype(str) + '_' + test['part_nm'].astype(str) + '_' + test['str_nm'].astype(str) + '_' + test['team_nm'].astype(str) + '_' + test['buyer_nm'].astype(str)\n",
    "\n",
    "### Make corpus\n",
    "p_level = 'customer_info'  # 상품 분류 수준\n",
    "\n",
    "# W2V 학습데이터가 부족하여 구매한 상품 목록으로부터 n배 oversampling을 수행\n",
    "def oversample(x, n, seed=0):\n",
    "    if n == 0:\n",
    "        return list(x)\n",
    "    uw = np.unique(x)\n",
    "    bs = np.array([])\n",
    "    np.random.seed(seed)\n",
    "    for j in range(n):\n",
    "        bs = np.append(bs, np.random.choice(uw, len(uw), replace=True))\n",
    "    return list(bs)\n",
    "\n",
    "train_corpus = list(train.groupby('custid')[p_level].agg(oversample, 20))\n",
    "test_corpus = list(test.groupby('custid')[p_level].agg(oversample, 20))\n",
    "\n",
    "\n",
    "### Training the Word2Vec model\n",
    "num_features = 100 # 단어 벡터 차원 수\n",
    "min_word_count = 1 # 최소 단어 수\n",
    "context = 5 # 학습 윈도우(인접한 단어 리스트) 크기\n",
    "\n",
    "# 초기화 및 모델 학습\n",
    "from gensim.models import word2vec\n",
    "\n",
    "# 모델 학습\n",
    "w2v = word2vec.Word2Vec(train_corpus, \n",
    "                        vector_size=num_features, \n",
    "                        min_count=min_word_count,\n",
    "                        window=context,\n",
    "                        seed=0, workers=1)\n",
    "# 필요없는 메모리 unload\n",
    "w2v.init_sims(replace=True)\n",
    "\n",
    "\n",
    "### Make features\n",
    "# 구매상품에 해당하는 벡터의 평균/최소/최대 벡터를 feature로 만드는 전처리기\n",
    "class EmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.dim = num_features\n",
    "    def fit(self, X):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.hstack([\n",
    "                np.max([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0),\n",
    "                np.min([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0),\n",
    "                np.mean([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0),                \n",
    "                #np.std([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0)                \n",
    "            ]) \n",
    "            for words in X\n",
    "        ]) \n",
    "\n",
    "# W2V 기반 feature 생성\n",
    "train_features = pd.DataFrame(EmbeddingVectorizer(w2v.wv).fit(train_corpus).transform(train_corpus))\n",
    "test_features = pd.DataFrame(EmbeddingVectorizer(w2v.wv).transform(test_corpus))\n",
    "\n",
    "train_features.columns = ['v'+f'{c+1:03d}' for c in train_features.columns]\n",
    "test_features.columns = ['v'+f'{c+1:03d}' for c in test_features.columns]\n",
    "\n",
    "# 학습용과 제출용 데이터로 분리\n",
    "X_train_customer_info = pd.concat([pd.DataFrame({'custid': np.sort(train['custid'].unique())}), train_features], axis=1)#.to_csv('X_train_buyer.csv', index=False)\n",
    "X_test_customer_info = pd.concat([pd.DataFrame({'custid': np.sort(test['custid'].unique())}), test_features], axis=1)#.to_csv('X_test_buyer.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b9f951f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run word2vec_customer_info.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c60afc0",
   "metadata": {},
   "source": [
    "### Feature Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0c41f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train_corner['custid']\n",
    "del X_test_corner['custid']\n",
    "del X_train_brd['custid']\n",
    "del X_test_brd['custid']\n",
    "del X_train_pc['custid']\n",
    "del X_test_pc['custid']\n",
    "del X_train_part['custid']\n",
    "del X_test_part['custid']\n",
    "del X_train_customer_info['custid']\n",
    "del X_test_customer_info['custid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b32b2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_corner.columns = X_train_corner.columns.map(lambda x : \"corner_\" + str(x))\n",
    "X_test_corner.columns = X_test_corner.columns.map(lambda x : \"corner_\" + str(x))\n",
    "X_train_brd.columns = X_train_brd.columns.map(lambda x : \"brd_\" + str(x))\n",
    "X_test_brd.columns = X_test_brd.columns.map(lambda x : \"brd_\" + str(x))\n",
    "X_train_pc.columns = X_train_pc.columns.map(lambda x : \"pc_\" + str(x))\n",
    "X_test_pc.columns = X_test_pc.columns.map(lambda x : \"pc_\" + str(x))\n",
    "X_train_part.columns = X_train_part.columns.map(lambda x : \"part_\" + str(x))\n",
    "X_test_part.columns = X_test_part.columns.map(lambda x : \"part_\" + str(x))\n",
    "X_train_customer_info.columns = X_train_customer_info.columns.map(lambda x : \"customer_info_\" + str(x))\n",
    "X_test_customer_info.columns = X_test_customer_info.columns.map(lambda x : \"customer_info_\" + str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3b857f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corner_v001</th>\n",
       "      <th>corner_v002</th>\n",
       "      <th>corner_v003</th>\n",
       "      <th>corner_v004</th>\n",
       "      <th>corner_v005</th>\n",
       "      <th>corner_v006</th>\n",
       "      <th>corner_v007</th>\n",
       "      <th>corner_v008</th>\n",
       "      <th>corner_v009</th>\n",
       "      <th>corner_v010</th>\n",
       "      <th>...</th>\n",
       "      <th>customer_info_v291</th>\n",
       "      <th>customer_info_v292</th>\n",
       "      <th>customer_info_v293</th>\n",
       "      <th>customer_info_v294</th>\n",
       "      <th>customer_info_v295</th>\n",
       "      <th>customer_info_v296</th>\n",
       "      <th>customer_info_v297</th>\n",
       "      <th>customer_info_v298</th>\n",
       "      <th>customer_info_v299</th>\n",
       "      <th>customer_info_v300</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.169340</td>\n",
       "      <td>0.206082</td>\n",
       "      <td>0.298115</td>\n",
       "      <td>0.215792</td>\n",
       "      <td>0.135647</td>\n",
       "      <td>0.222850</td>\n",
       "      <td>0.242829</td>\n",
       "      <td>0.179579</td>\n",
       "      <td>0.182147</td>\n",
       "      <td>0.186440</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.037729</td>\n",
       "      <td>-0.001512</td>\n",
       "      <td>-0.020845</td>\n",
       "      <td>-0.055514</td>\n",
       "      <td>0.017132</td>\n",
       "      <td>-0.006139</td>\n",
       "      <td>0.055492</td>\n",
       "      <td>0.009044</td>\n",
       "      <td>-0.011974</td>\n",
       "      <td>0.072268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.146145</td>\n",
       "      <td>0.206082</td>\n",
       "      <td>0.242883</td>\n",
       "      <td>0.196997</td>\n",
       "      <td>0.187296</td>\n",
       "      <td>0.205009</td>\n",
       "      <td>0.242829</td>\n",
       "      <td>0.179579</td>\n",
       "      <td>0.161699</td>\n",
       "      <td>0.103792</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029017</td>\n",
       "      <td>0.010989</td>\n",
       "      <td>-0.014123</td>\n",
       "      <td>-0.060092</td>\n",
       "      <td>-0.038468</td>\n",
       "      <td>0.017699</td>\n",
       "      <td>0.006862</td>\n",
       "      <td>0.016091</td>\n",
       "      <td>-0.016063</td>\n",
       "      <td>0.006584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.136681</td>\n",
       "      <td>0.206082</td>\n",
       "      <td>0.132561</td>\n",
       "      <td>0.196997</td>\n",
       "      <td>0.189242</td>\n",
       "      <td>0.109701</td>\n",
       "      <td>0.242829</td>\n",
       "      <td>0.182884</td>\n",
       "      <td>0.196261</td>\n",
       "      <td>0.134575</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011061</td>\n",
       "      <td>-0.091687</td>\n",
       "      <td>-0.007593</td>\n",
       "      <td>-0.009291</td>\n",
       "      <td>0.020926</td>\n",
       "      <td>-0.007941</td>\n",
       "      <td>0.048019</td>\n",
       "      <td>-0.000254</td>\n",
       "      <td>-0.034819</td>\n",
       "      <td>0.027352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.039630</td>\n",
       "      <td>-0.055562</td>\n",
       "      <td>0.011438</td>\n",
       "      <td>0.183936</td>\n",
       "      <td>0.084797</td>\n",
       "      <td>-0.082874</td>\n",
       "      <td>0.037034</td>\n",
       "      <td>-0.000180</td>\n",
       "      <td>-0.037953</td>\n",
       "      <td>-0.123760</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002802</td>\n",
       "      <td>-0.127186</td>\n",
       "      <td>-0.027822</td>\n",
       "      <td>-0.010264</td>\n",
       "      <td>0.022178</td>\n",
       "      <td>0.041904</td>\n",
       "      <td>0.092767</td>\n",
       "      <td>-0.097941</td>\n",
       "      <td>-0.106284</td>\n",
       "      <td>0.113548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.045659</td>\n",
       "      <td>0.206082</td>\n",
       "      <td>-0.008510</td>\n",
       "      <td>0.196997</td>\n",
       "      <td>0.042490</td>\n",
       "      <td>-0.004797</td>\n",
       "      <td>0.242829</td>\n",
       "      <td>0.011994</td>\n",
       "      <td>0.088647</td>\n",
       "      <td>0.116853</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025847</td>\n",
       "      <td>-0.056588</td>\n",
       "      <td>0.014993</td>\n",
       "      <td>0.003065</td>\n",
       "      <td>-0.015731</td>\n",
       "      <td>-0.047955</td>\n",
       "      <td>-0.028582</td>\n",
       "      <td>0.035733</td>\n",
       "      <td>0.004416</td>\n",
       "      <td>0.040190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14375</th>\n",
       "      <td>0.222973</td>\n",
       "      <td>0.206082</td>\n",
       "      <td>0.107754</td>\n",
       "      <td>0.196997</td>\n",
       "      <td>0.004659</td>\n",
       "      <td>-0.004797</td>\n",
       "      <td>0.242829</td>\n",
       "      <td>0.011994</td>\n",
       "      <td>0.185086</td>\n",
       "      <td>0.147618</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010465</td>\n",
       "      <td>0.102772</td>\n",
       "      <td>-0.155550</td>\n",
       "      <td>0.047154</td>\n",
       "      <td>-0.024446</td>\n",
       "      <td>0.066425</td>\n",
       "      <td>0.092122</td>\n",
       "      <td>0.036953</td>\n",
       "      <td>0.009783</td>\n",
       "      <td>-0.008029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14376</th>\n",
       "      <td>-0.045659</td>\n",
       "      <td>0.206082</td>\n",
       "      <td>-0.121564</td>\n",
       "      <td>0.196997</td>\n",
       "      <td>-0.042386</td>\n",
       "      <td>-0.004797</td>\n",
       "      <td>0.242829</td>\n",
       "      <td>0.011994</td>\n",
       "      <td>-0.037777</td>\n",
       "      <td>-0.133329</td>\n",
       "      <td>...</td>\n",
       "      <td>0.108564</td>\n",
       "      <td>-0.118474</td>\n",
       "      <td>-0.002553</td>\n",
       "      <td>-0.051129</td>\n",
       "      <td>-0.111259</td>\n",
       "      <td>0.010459</td>\n",
       "      <td>0.093976</td>\n",
       "      <td>-0.016191</td>\n",
       "      <td>0.038766</td>\n",
       "      <td>0.060395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14377</th>\n",
       "      <td>-0.045659</td>\n",
       "      <td>0.206082</td>\n",
       "      <td>0.108535</td>\n",
       "      <td>0.226596</td>\n",
       "      <td>-0.022717</td>\n",
       "      <td>0.048247</td>\n",
       "      <td>0.242829</td>\n",
       "      <td>0.076995</td>\n",
       "      <td>-0.014775</td>\n",
       "      <td>0.028709</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.100094</td>\n",
       "      <td>-0.043826</td>\n",
       "      <td>0.016747</td>\n",
       "      <td>-0.066141</td>\n",
       "      <td>0.069784</td>\n",
       "      <td>-0.024194</td>\n",
       "      <td>0.078990</td>\n",
       "      <td>0.060629</td>\n",
       "      <td>0.000117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14378</th>\n",
       "      <td>0.145338</td>\n",
       "      <td>0.173095</td>\n",
       "      <td>0.074276</td>\n",
       "      <td>0.135753</td>\n",
       "      <td>0.012448</td>\n",
       "      <td>0.099252</td>\n",
       "      <td>0.147267</td>\n",
       "      <td>0.202321</td>\n",
       "      <td>0.142460</td>\n",
       "      <td>0.147618</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001240</td>\n",
       "      <td>-0.077764</td>\n",
       "      <td>-0.086327</td>\n",
       "      <td>-0.069067</td>\n",
       "      <td>0.078929</td>\n",
       "      <td>0.092216</td>\n",
       "      <td>0.018089</td>\n",
       "      <td>-0.046489</td>\n",
       "      <td>0.066639</td>\n",
       "      <td>0.096389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14379</th>\n",
       "      <td>0.005646</td>\n",
       "      <td>0.206082</td>\n",
       "      <td>-0.036015</td>\n",
       "      <td>0.196997</td>\n",
       "      <td>-0.006060</td>\n",
       "      <td>0.057248</td>\n",
       "      <td>0.242829</td>\n",
       "      <td>0.025219</td>\n",
       "      <td>0.097758</td>\n",
       "      <td>-0.016012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003770</td>\n",
       "      <td>0.001480</td>\n",
       "      <td>0.062021</td>\n",
       "      <td>-0.125061</td>\n",
       "      <td>0.034100</td>\n",
       "      <td>0.005533</td>\n",
       "      <td>0.015059</td>\n",
       "      <td>-0.024923</td>\n",
       "      <td>0.072858</td>\n",
       "      <td>0.106149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14380 rows × 1950 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       corner_v001  corner_v002  corner_v003  corner_v004  corner_v005  \\\n",
       "0         0.169340     0.206082     0.298115     0.215792     0.135647   \n",
       "1         0.146145     0.206082     0.242883     0.196997     0.187296   \n",
       "2         0.136681     0.206082     0.132561     0.196997     0.189242   \n",
       "3        -0.039630    -0.055562     0.011438     0.183936     0.084797   \n",
       "4        -0.045659     0.206082    -0.008510     0.196997     0.042490   \n",
       "...            ...          ...          ...          ...          ...   \n",
       "14375     0.222973     0.206082     0.107754     0.196997     0.004659   \n",
       "14376    -0.045659     0.206082    -0.121564     0.196997    -0.042386   \n",
       "14377    -0.045659     0.206082     0.108535     0.226596    -0.022717   \n",
       "14378     0.145338     0.173095     0.074276     0.135753     0.012448   \n",
       "14379     0.005646     0.206082    -0.036015     0.196997    -0.006060   \n",
       "\n",
       "       corner_v006  corner_v007  corner_v008  corner_v009  corner_v010  ...  \\\n",
       "0         0.222850     0.242829     0.179579     0.182147     0.186440  ...   \n",
       "1         0.205009     0.242829     0.179579     0.161699     0.103792  ...   \n",
       "2         0.109701     0.242829     0.182884     0.196261     0.134575  ...   \n",
       "3        -0.082874     0.037034    -0.000180    -0.037953    -0.123760  ...   \n",
       "4        -0.004797     0.242829     0.011994     0.088647     0.116853  ...   \n",
       "...            ...          ...          ...          ...          ...  ...   \n",
       "14375    -0.004797     0.242829     0.011994     0.185086     0.147618  ...   \n",
       "14376    -0.004797     0.242829     0.011994    -0.037777    -0.133329  ...   \n",
       "14377     0.048247     0.242829     0.076995    -0.014775     0.028709  ...   \n",
       "14378     0.099252     0.147267     0.202321     0.142460     0.147618  ...   \n",
       "14379     0.057248     0.242829     0.025219     0.097758    -0.016012  ...   \n",
       "\n",
       "       customer_info_v291  customer_info_v292  customer_info_v293  \\\n",
       "0               -0.037729           -0.001512           -0.020845   \n",
       "1               -0.029017            0.010989           -0.014123   \n",
       "2                0.011061           -0.091687           -0.007593   \n",
       "3               -0.002802           -0.127186           -0.027822   \n",
       "4                0.025847           -0.056588            0.014993   \n",
       "...                   ...                 ...                 ...   \n",
       "14375            0.010465            0.102772           -0.155550   \n",
       "14376            0.108564           -0.118474           -0.002553   \n",
       "14377            0.000200            0.100094           -0.043826   \n",
       "14378            0.001240           -0.077764           -0.086327   \n",
       "14379            0.003770            0.001480            0.062021   \n",
       "\n",
       "       customer_info_v294  customer_info_v295  customer_info_v296  \\\n",
       "0               -0.055514            0.017132           -0.006139   \n",
       "1               -0.060092           -0.038468            0.017699   \n",
       "2               -0.009291            0.020926           -0.007941   \n",
       "3               -0.010264            0.022178            0.041904   \n",
       "4                0.003065           -0.015731           -0.047955   \n",
       "...                   ...                 ...                 ...   \n",
       "14375            0.047154           -0.024446            0.066425   \n",
       "14376           -0.051129           -0.111259            0.010459   \n",
       "14377            0.016747           -0.066141            0.069784   \n",
       "14378           -0.069067            0.078929            0.092216   \n",
       "14379           -0.125061            0.034100            0.005533   \n",
       "\n",
       "       customer_info_v297  customer_info_v298  customer_info_v299  \\\n",
       "0                0.055492            0.009044           -0.011974   \n",
       "1                0.006862            0.016091           -0.016063   \n",
       "2                0.048019           -0.000254           -0.034819   \n",
       "3                0.092767           -0.097941           -0.106284   \n",
       "4               -0.028582            0.035733            0.004416   \n",
       "...                   ...                 ...                 ...   \n",
       "14375            0.092122            0.036953            0.009783   \n",
       "14376            0.093976           -0.016191            0.038766   \n",
       "14377           -0.024194            0.078990            0.060629   \n",
       "14378            0.018089           -0.046489            0.066639   \n",
       "14379            0.015059           -0.024923            0.072858   \n",
       "\n",
       "       customer_info_v300  \n",
       "0                0.072268  \n",
       "1                0.006584  \n",
       "2                0.027352  \n",
       "3                0.113548  \n",
       "4                0.040190  \n",
       "...                   ...  \n",
       "14375           -0.008029  \n",
       "14376            0.060395  \n",
       "14377            0.000117  \n",
       "14378            0.096389  \n",
       "14379            0.106149  \n",
       "\n",
       "[14380 rows x 1950 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_features_train = pd.concat([X_train_corner, X_train_brd, X_train_pc, X_train_part, X_train_customer_info], axis=1) ; \n",
    "w2v_features_test = pd.concat([X_test_corner, X_test_brd, X_test_pc, X_test_part, X_test_customer_info], axis=1) ; w2v_features_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6e77cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_features_train.to_csv('seo_w2v_features_train.csv', index=False,encoding = 'utf-8')\n",
    "w2v_features_test.to_csv('seo_w2v_features_test.csv', index=False,encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b355d24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
